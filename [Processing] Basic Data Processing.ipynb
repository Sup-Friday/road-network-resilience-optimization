{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington_DC\n"
     ]
    }
   ],
   "source": [
    "import shuo\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from shuo.basics import FileNames, PickleDataWriter as pdw, UnionFind, FilesReader\n",
    "\n",
    "File_Prefixes = ['Washington_DC', \n",
    "                 'Seattle_city_WA',\n",
    "                 'Chicago_city_IL',\n",
    "                 'Madison_county_AL', \n",
    "                 'Mobile_city_AL', \n",
    "                 'Napa_city_CA', \n",
    "                 'Redding_city_CA',\n",
    "                 'Yuma_county_AZ']\n",
    "budget = 0\n",
    "current_index = 0\n",
    "folder = 'data_processed'\n",
    "file_prefix = File_Prefixes[current_index]\n",
    "print(file_prefix)\n",
    "\n",
    "N = FileNames(folder, file_prefix, budget)\n",
    "F = FilesReader(folder, file_prefix, budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 10095 Edges: 16401\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load basic data\n",
    "    1. Original graph data: undirected road network graph\n",
    "    2. TAZ (Traffic Analysis Zone) data: set of polygons partioning the whole area\n",
    "    3. Travel demand data: based on TAZ, whose entry (i, j) stores the travel volume from TAZ i to TAZ j\n",
    "    4. Flood zone data: set of polygons with flooding info\n",
    "'''\n",
    "\n",
    "# Load original graph data\n",
    "OG = nx.read_graphml(N.graph())\n",
    "print('Nodes:', len(OG.nodes), 'Edges:', len(OG.edges))\n",
    "\n",
    "# Load TAZ data\n",
    "zone_df = gpd.read_file(N.zone())\n",
    "\n",
    "# Load travel demand data (based on TAZ)\n",
    "demand_mat = F.travel_demand_matrix()\n",
    "\n",
    "# Load flood zone data\n",
    "flood_df = gpd.read_file(N.flood_zone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood polygons: 202\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute flooded road network graph\n",
    "'''\n",
    "\n",
    "# Build flood polygons list\n",
    "flood_polygons = []\n",
    "for index, row in flood_df.iterrows():\n",
    "    # If use FEMA data\n",
    "    if row['ZONE_SUBTY'] == '0.2 PCT ANNUAL CHANCE FLOOD HAZARD':\n",
    "    # If use EnviroAtlas data\n",
    "    #if row['Flooded'] == 1:\n",
    "        flood_polygons.append(row['geometry'])\n",
    "print('Flood polygons:', len(flood_polygons))\n",
    "\n",
    "# Compute and save flooded graph\n",
    "recompute = False\n",
    "if recompute:\n",
    "    FG = shuo.graph_with_flood_info(OG, flood_polygons)[0]\n",
    "    nx.write_graphml(FG, N.graph(-1))\n",
    "    UG, AG = shuo.unaffected_and_affected_graph(FG)\n",
    "    nx.write_graphml(UG, N.graph(1))\n",
    "    nx.write_graphml(AG, N.graph(2))\n",
    "else:\n",
    "    FG = nx.read_graphml(N.graph(-1))\n",
    "    UG = nx.read_graphml(N.graph(1))\n",
    "    AG = nx.read_graphml(N.graph(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute Mapping\n",
    "    1. Btw indices and nodes\n",
    "    2. Btw indices and zones\n",
    "    3. Btw zones and nodes\n",
    "    4. Btw zone indices and node indices\n",
    "'''\n",
    "\n",
    "# Indices and nodes mapping\n",
    "recompute = False\n",
    "if recompute:\n",
    "    dict_index_to_node, dict_node_to_index = shuo.indices_and_nodes_mapping(OG)\n",
    "    pdw(N.mapping(0)).write_with_truncating(dict_index_to_node)\n",
    "    pdw(N.mapping(0)).write(dict_node_to_index)\n",
    "else:\n",
    "    dict_index_to_node, dict_node_to_index = F.mapping(0)\n",
    "    \n",
    "# Indices and zones mapping\n",
    "recompute = False\n",
    "if recompute:\n",
    "    dict_index_to_zone, dict_zone_to_index = shuo.indices_and_zones_mapping_from_GDF(zone_df, 'TAZ')\n",
    "    pdw(N.mapping(1)).write_with_truncating(dict_index_to_zone)\n",
    "    pdw(N.mapping(1)).write(dict_zone_to_index)\n",
    "else:\n",
    "    dict_index_to_zone, dict_zone_to_index = F.mapping(1)\n",
    "    \n",
    "# Compute and save zones and nodes mapping\n",
    "recompute = False\n",
    "if recompute:\n",
    "    dict_zone_to_nodes, dict_node_to_zones = shuo.zones_and_nodes_mapping_from_GDF(OG, zone_df, 'TAZ')\n",
    "    pdw(N.mapping(2)).write_with_truncating(dict_zone_to_nodes)\n",
    "    pdw(N.mapping(2)).write(dict_node_to_zones)\n",
    "else:\n",
    "    dict_zone_to_nodes, dict_node_to_zones = F.mapping(2)\n",
    "    \n",
    "# Compute and save zone indices and node indices mapping\n",
    "recompute = False\n",
    "if recompute:\n",
    "    dict_zone_ind_to_node_incs, dict_node_ind_to_zone_incs = shuo.zone_indices_and_node_indices_mapping_from_GDF(OG, zone_df)\n",
    "    pdw(N.mapping(3)).write_with_truncating(dict_zone_ind_to_node_incs)\n",
    "    pdw(N.mapping(3)).write(dict_node_ind_to_zone_incs)\n",
    "else:\n",
    "    dict_zone_ind_to_node_incs, dict_node_ind_to_zone_incs = F.mapping(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute node and zone distances matrix along with trips counts matrix \n",
    "'''\n",
    "\n",
    "# Compute and save node distance matrix\n",
    "recompute = False\n",
    "if recompute:\n",
    "    node_dist_mat = shuo.nodes_distances(OG, dict_node_to_index)\n",
    "    np.save(N.node_distance_matrix(), node_dist_mat)\n",
    "else:\n",
    "    node_dist_mat = F.node_distance_matrix()\n",
    "    \n",
    "# Compute and save zone distance and trips count matrix\n",
    "recompute = False\n",
    "if recompute:\n",
    "    trips_count_mat = shuo.zones_distances_and_trips_count(node_dist_mat, dict_index_to_node, dict_node_to_zones, dict_zone_to_index)[1]\n",
    "    np.save(N.zone_trips_count_matrix(), trips_count_mat)\n",
    "else:\n",
    "    trips_count_mat = F.zone_trips_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute node and zone distances matrix along with trips counts matrix under flooding\n",
    "'''\n",
    "\n",
    "# Compute and save node distance matrix under flooding\n",
    "recompute = False\n",
    "if recompute:\n",
    "    node_dist_mat = shuo.nodes_distances(UG, dict_node_to_index)\n",
    "    np.save(N.node_distance_matrix(1), node_dist_mat)\n",
    "else:\n",
    "    node_dist_mat = F.node_distance_matrix(1)\n",
    "    \n",
    "# Compute and save zone distance and trips count matrix under flooding\n",
    "recompute = False\n",
    "if recompute:\n",
    "    trips_count_mat = shuo.zones_distances_and_trips_count(node_dist_mat, dict_index_to_node, dict_node_to_zones, dict_zone_to_index)[1]\n",
    "    np.save(N.zone_trips_count_matrix(1), trips_count_mat)\n",
    "else:\n",
    "    trips_count_mat = F.zone_trips_count_matrix(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in compressed graph: 284 Candidate edges: 441\n",
      "Original travel demand: 644337.6419999999 Compressed travel demand: 636061.5209999442\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute compressed graphs and travel demand matrix\n",
    "'''\n",
    "\n",
    "# Compute and save compressed unaffected and affected graph along with travel demand matrix\n",
    "recompute = False\n",
    "if recompute:\n",
    "    CUG, CAG, compressed_demand_mat = shuo.compressed_graph(UG, AG, (trips_count_mat[0] + trips_count_mat[1]), demand_mat, (dict_node_to_index, dict_zone_ind_to_node_incs))\n",
    "    nx.write_graphml(CUG, N.compressed_graph(1))\n",
    "    nx.write_graphml(CAG, N.compressed_graph(2))\n",
    "    np.save(N.travel_demand_matrix(1), compressed_demand_mat)\n",
    "else:\n",
    "    CUG = F.compressed_graph(1)\n",
    "    CAG = F.compressed_graph(2)\n",
    "    compressed_demand_mat = F.travel_demand_matrix(1)\n",
    "\n",
    "print('Nodes in compressed graph:', compressed_demand_mat.shape[0], 'Candidate edges:', len(CAG.edges))\n",
    "print('Original travel demand:', np.sum(demand_mat), 'Compressed travel demand:', np.sum(compressed_demand_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Functionalities for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 15700 unaffected edges and 701 affected edges\n",
      "Ratio of affected edges to total edges: 4.27\n",
      "There're 284 connected components\n",
      "There're 8 connected components of size > 5:\n",
      "[7505, 2155, 34, 27, 16, 13, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "# Print some info of flooded graph\n",
    "print('There\\'re', len(UG.edges), 'unaffected edges and', len(AG.edges), 'affected edges\\nRatio of affected edges to total edges:', round((len(AG.edges) * 100) / len(FG.edges), 2))\n",
    "\n",
    "uf = UnionFind(UG.nodes, UG.edges)\n",
    "children = uf.children()\n",
    "\n",
    "uf.show_info()\n",
    "children_counts = []\n",
    "small_CC_num = 0\n",
    "for key, value in children.items():\n",
    "    num = len(value)\n",
    "    if num > 5:\n",
    "        children_counts.append(num)\n",
    "    else:\n",
    "        small_CC_num += 1\n",
    "children_counts.sort(reverse = True)\n",
    "\n",
    "print('There\\'re', len(children_counts), 'connected components of size > 5:')\n",
    "print(children_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a shapefile based on graphml\n",
    "import fiona\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely.geometry import LineString, mapping\n",
    "from shuo.basics import get_LineString\n",
    "\n",
    "# Graphml to construct a shapefile\n",
    "G = F.graph(-1)\n",
    "\n",
    "# Output file path\n",
    "file_path = folder + '/flooded.shp'\n",
    "\n",
    "# Convert the nodes of road network into an easy-to-read format\n",
    "points = {node: [float(data['x']), float(data['y'])] for node, data in G.nodes(data=True)}\n",
    "\n",
    "# Create shapefile of colored edges\n",
    "schema = {'geometry': 'LineString', 'properties':{'flood distance': 'float'}}\n",
    "with fiona.open(file_path, 'w', 'ESRI Shapefile', schema, fiona.crs.from_epsg(4326)) as F:\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        if d['flooded_distance'] == 0: continue\n",
    "        elem = dict()\n",
    "        line = get_LineString(d['geometry']) if 'geometry' in d else LineString([points[u], points[v]])\n",
    "        elem['geometry'] = mapping(line)\n",
    "        elem['properties'] = dict()\n",
    "        elem['properties']['flood distance'] = d['flooded_distance']\n",
    "\n",
    "        F.write(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
